import logging
import platform
import sys
from typing import Annotated

import pandas as pd
from fastapi import APIRouter, Depends, HTTPException, Request

from api.dependencies import get_vector_store
from api.models import (
    AdminVersionInfo,
    ExcelSheetsRequest,
    ExcelSheetsResponse,
    HealthCheck,
    IngestRequest,
    IngestResponse,
    NotificationsAdminStats,
    PortalAdaptersResponse,
    PortalFiltersRequest,
    PortalIngestResponse,
    ReindexRequest,
    ReindexResponse,
)
from config.settings import settings
from data.csv_loader import DataLoaderCsv, DataLoaderExcel
from data.schemas import Property, PropertyCollection
from notifications.alert_storage_stats import load_alert_storage_summary
from utils.property_cache import load_collection, save_collection
from vector_store.chroma_store import ChromaPropertyStore

logger = logging.getLogger(__name__)

router = APIRouter(tags=["Admin"])


def _format_python_version() -> str:
    return f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}"


@router.get("/admin/version", response_model=AdminVersionInfo)
async def admin_version_info() -> AdminVersionInfo:
    return AdminVersionInfo(
        version=settings.version,
        environment=settings.environment,
        app_title=settings.app_title,
        python_version=_format_python_version(),
        platform=platform.platform(),
    )


@router.post("/admin/ingest", response_model=IngestResponse)
async def ingest_data(request: IngestRequest):
    """
    Trigger data ingestion from URLs.
    Downloads CSV/Excel files, processes them, and saves to local cache.
    Does NOT automatically reindex vector store (call /reindex for that).
    Enforces max_properties limit from settings.
    """
    urls = request.file_urls or settings.default_datasets
    if not urls:
        raise HTTPException(status_code=400, detail="No URLs provided and no defaults configured")

    try:
        all_properties = []
        errors = []
        max_props = settings.max_props

        for url in urls:
            try:
                # Detect source type and choose appropriate loader
                source_type = DataLoaderExcel.detect_source_type(url)
                source_name = request.source_name or url

                if source_type == "excel":
                    loader = DataLoaderExcel(
                        url,
                        sheet_name=request.sheet_name,
                        header_row=request.header_row,
                        source_type="excel",
                    )
                else:
                    loader = DataLoaderCsv(url)

                df = loader.load_df()
                # Enforce max_properties limit via rows_count parameter
                # Calculate remaining capacity to stay within limit
                remaining_capacity = max(0, max_props - len(all_properties))
                df_formatted = loader.load_format_df(df, rows_count=remaining_capacity)

                # Convert to Property objects
                # We use to_dict('records') and validate with Pydantic
                records = df_formatted.to_dict(orient="records")
                props = []
                for record in records:
                    try:
                        # Add source tracking to each property
                        if "source_url" not in record or pd.isna(record.get("source_url")):
                            record["source_url"] = source_name
                        if "source_platform" not in record or pd.isna(
                            record.get("source_platform")
                        ):
                            record["source_platform"] = source_type
                        props.append(Property(**record))
                    except Exception:
                        # Skip invalid records but log?
                        pass

                all_properties.extend(props)
                logger.info(f"Loaded {len(props)} properties from {url}")

                # Stop if we've reached the limit
                if len(all_properties) >= max_props:
                    logger.warning(
                        f"Reached maximum property limit ({max_props}), stopping ingestion"
                    )
                    break
            except Exception as e:
                msg = f"Failed to load {url}: {str(e)}"
                logger.error(msg)
                errors.append(msg)

        if not all_properties:
            raise HTTPException(status_code=500, detail="No properties could be loaded")

        # Get source type from first property (all from same source in this implementation)
        source_type_val = None
        source_name_val = None
        if all_properties:
            if all_properties[0].source_platform:
                source_type_val = all_properties[0].source_platform
            if all_properties[0].source_url:
                source_name_val = all_properties[0].source_url

        collection = PropertyCollection(
            properties=all_properties,
            total_count=len(all_properties),
            source=source_name_val,
            source_type=source_type_val,
        )
        save_collection(collection)

        message = "Ingestion successful"
        if len(all_properties) >= max_props:
            message += f" (reached maximum property limit of {max_props})"

        return IngestResponse(
            message=message,
            properties_processed=len(all_properties),
            errors=errors,
            source_type=source_type_val,
            source_name=source_name_val,
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Ingestion failed: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@router.post("/admin/excel/sheets", response_model=ExcelSheetsResponse)
async def get_excel_sheets(request: ExcelSheetsRequest):
    """
    Get sheet names from an Excel file.

    Returns available sheets and their row counts for sheet selection UI.
    """
    try:
        loader = DataLoaderExcel(request.file_url)
        sheet_names = loader.get_sheet_names()
        row_counts = {}

        # Get row count for each sheet
        for sheet in sheet_names:
            try:
                sheet_loader = DataLoaderExcel(
                    request.file_url, sheet_name=sheet, source_type="excel"
                )
                df = sheet_loader.load_df()
                row_counts[sheet] = len(df)
            except Exception as e:
                logger.warning(f"Could not read sheet '{sheet}': {e}")
                row_counts[sheet] = 0

        # Determine default sheet (first non-empty sheet)
        default_sheet = None
        for sheet, count in row_counts.items():
            if count > 0:
                default_sheet = sheet
                break
        if not default_sheet and sheet_names:
            default_sheet = sheet_names[0]

        return ExcelSheetsResponse(
            file_url=request.file_url,
            sheet_names=sheet_names,
            default_sheet=default_sheet,
            row_count=row_counts,
        )
    except ImportError as e:
        raise HTTPException(
            status_code=400,
            detail=f"Excel libraries not available: {str(e)}",
        ) from e
    except Exception as e:
        logger.error(f"Failed to get Excel sheets: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@router.post("/admin/reindex", response_model=ReindexResponse)
async def reindex_data(
    request: ReindexRequest,
    store: Annotated[ChromaPropertyStore, Depends(get_vector_store)],
):
    """
    Reindex data from cache to vector store.
    """
    collection = load_collection()
    if not collection:
        raise HTTPException(
            status_code=404,
            detail="No data in cache. Run ingestion first.",
        )

    try:
        # In a real scenario, we might want to clear the collection first if
        # request.clear_existing is True.
        # Currently ChromaPropertyStore doesn't expose a clear method publicly in the
        # interface we checked.
        # We will just add documents (upsert behavior usually).

        if not store:
            raise HTTPException(status_code=503, detail="Vector store unavailable")

        store.add_documents(collection.properties)

        return ReindexResponse(message="Reindexing successful", count=len(collection.properties))
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Reindexing failed: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@router.get("/admin/health", response_model=HealthCheck)
async def admin_health_check(
    store: Annotated[ChromaPropertyStore, Depends(get_vector_store)],
):
    """
    Detailed health check for admin.
    """
    status = "healthy"

    # Check cache
    if not load_collection():
        status = "degraded (no data cache)"

    # Check vector store
    if not store:
        status = "degraded (vector store unavailable)"

    return HealthCheck(status=status, version=settings.version)


@router.get("/admin/metrics", response_model=dict)
async def admin_metrics(request: Request):
    """
    Return simple API metrics.
    """
    try:
        metrics = getattr(request.app.state, "metrics", {})
        return dict(metrics)
    except Exception as e:
        logger.error(f"Metrics retrieval failed: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@router.get("/admin/notifications-stats", response_model=NotificationsAdminStats)
async def admin_notifications_stats(request: Request):
    try:
        scheduler = getattr(request.app.state, "scheduler", None)
        scheduler_running = False
        alerts_storage_path = ".alerts"

        if scheduler is not None:
            if hasattr(scheduler, "_thread") and scheduler._thread is not None:
                scheduler_running = bool(scheduler._thread.is_alive())
            if hasattr(scheduler, "_storage_path_alerts"):
                alerts_storage_path = str(scheduler._storage_path_alerts)

        summary = load_alert_storage_summary(alerts_storage_path)

        return NotificationsAdminStats(
            scheduler_running=scheduler_running,
            alerts_storage_path=alerts_storage_path,
            sent_alerts_total=int(summary.sent_total),
            pending_alerts_total=int(summary.pending_total),
            pending_alerts_by_type=dict(summary.pending_by_type),
            pending_alerts_oldest_created_at=summary.pending_oldest_created_at,
            pending_alerts_newest_created_at=summary.pending_newest_created_at,
        )
    except Exception as e:
        logger.error("Notifications stats retrieval failed: %s", e)
        raise HTTPException(status_code=500, detail=str(e)) from e


# Portal/API Integration Endpoints for TASK-006
@router.get("/admin/portals", response_model=PortalAdaptersResponse)
async def list_portals():
    """
    List all available portal adapters.

    Returns information about each portal including:
    - Whether it's configured (has API key if required)
    - Rate limit information
    """
    try:
        # Import here to avoid circular imports
        from data.adapters.registry import AdapterRegistry

        adapters_info = AdapterRegistry.get_all_info()

        return PortalAdaptersResponse(
            adapters=[info for info in adapters_info if info is not None],
            count=len([info for info in adapters_info if info is not None]),
        )
    except Exception as e:
        logger.error(f"Failed to list portals: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


@router.post("/admin/portals/fetch", response_model=PortalIngestResponse)
async def fetch_from_portal(request: PortalFiltersRequest):
    """
    Fetch property data from an external portal.

    Uses the specified portal adapter to fetch properties based on filters.
    The fetched data is automatically ingested into the property cache.
    """
    try:
        # Import here to avoid circular imports
        from data.adapters import get_adapter
        from data.adapters.base import PortalFilter

        # Get the adapter
        adapter = get_adapter(request.portal)
        if not adapter:
            available = ", ".join(_get_available_portal_names())
            raise HTTPException(
                status_code=404,
                detail=(
                    f"Portal adapter '{request.portal}' not found. Available portals: {available}"
                ),
            )

        # Build filters
        filters = PortalFilter(
            city=request.city,
            min_price=request.min_price,
            max_price=request.max_price,
            min_rooms=request.min_rooms,
            max_rooms=request.max_rooms,
            property_type=request.property_type,
            listing_type=request.listing_type,
            limit=request.limit,
        )

        # Fetch from portal
        result = adapter.fetch(filters)

        if not result.success:
            return PortalIngestResponse(
                success=False,
                message=f"Failed to fetch from portal: {'; '.join(result.errors)}",
                portal=request.portal,
                properties_processed=0,
                errors=result.errors,
                filters_applied=filters.to_dict(),
            )

        # Convert to Property objects
        all_properties = []
        errors = result.errors.copy()
        max_props = settings.max_props

        for record in result.properties:
            try:
                # Add source tracking
                if "source_url" not in record or not record.get("source_url"):
                    record["source_url"] = result.source
                if "source_platform" not in record or not record.get("source_platform"):
                    record["source_platform"] = result.source_type

                # Create Property object (will validate automatically)
                prop = Property(**record)
                all_properties.append(prop)

                # Stop if we've reached the limit
                if len(all_properties) >= max_props:
                    logger.warning(f"Reached maximum property limit ({max_props})")
                    break
            except Exception as e:
                msg = f"Failed to validate property: {str(e)}"
                logger.warning(msg)
                errors.append(msg)

        if not all_properties:
            return PortalIngestResponse(
                success=False,
                message="No valid properties could be fetched from portal",
                portal=request.portal,
                properties_processed=0,
                errors=errors,
                filters_applied=filters.to_dict(),
            )

        # Create collection and save
        source_name_val = request.source_name or f"{request.portal}_{filters.city or 'all'}"

        collection = PropertyCollection(
            properties=all_properties,
            total_count=len(all_properties),
            source=source_name_val,
            source_type=result.source_type,
        )
        save_collection(collection)

        message = f"Successfully fetched {len(all_properties)} properties from {request.portal}"
        if len(all_properties) >= max_props:
            message += f" (reached maximum property limit of {max_props})"

        return PortalIngestResponse(
            success=True,
            message=message,
            portal=request.portal,
            properties_processed=len(all_properties),
            source_type=result.source_type,
            source_name=source_name_val,
            errors=errors,
            filters_applied=filters.to_dict(),
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Portal fetch failed: {e}")
        raise HTTPException(status_code=500, detail=str(e)) from e


def _get_available_portal_names() -> list[str]:
    """Helper to get list of available portal names."""
    try:
        from data.adapters.registry import AdapterRegistry

        return AdapterRegistry.list_adapters()
    except Exception:
        return []
